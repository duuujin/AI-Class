# LSTM (Logn Shrot-Term Memory)
- 이름 그대로 “장단기기억”
- RNN의 한 종류로, 기존 RNN의 장기 의존성문제을 해결 하기위해 설계 됨
- “셀상태(Cell State)와 게이트(Gate)” 라는 독자적인 구조를 도입 하여 문제를 해결!
- 기존RNN
    - 은닉상태(h<sub>t</sub>) 하나만으로 정보를 전달
- LSTM
    - 은닉상태(h<sub>t</sub>) + 셀상태(c<sub>t</sub>)로 2가지 정보 전달 통로를 활용
    - 셀상태(c<sub>t</sub>)는 장기 기억을 전달하는 통로
    - 셀상태(c<sub>t</sub>)는 게이트(Gate)를 활용하여 장기 기억을 전달 할 지, 삭제할지를 결정

# LSTM 구성요소
- 셀 상태(Cell State)
    - 장기 기억 데이터를 전송 하는 통로
    - 기존 RNN의 은닉상태와는 다르게 덮어쓰기를 하지 않고, tanh함수를 통과하는등의 복잡한 변환이 일어나지 않음
    - 단순하고 직접적인 경로로써 기울기가 소실되지 않고 시간을 거슬러 올라갈 수 있게 함
- 게이트(Gate)
    - 장기 기억통로일 뿐인 “셀상태(Cell State)”를 실질적으로 컨트롤하는 중요한 역할
    - 어떤정보를 기억/망각/사용 할 지를 ‘스스로학습’하여 결정
    - 각기 다른 역할을 하는 3개의 게이트가 존재
        - 망각 게이트(Forget Gate)
        - 입력 게이트(Input Gate)
        - 출력 게이트(Output Gate)
        - 모든 게이트는 각각의 가중치(W<sub>f</sub>, W<sub>i</sub>, W<sub>o</sub>)를 가지고 있음

# LSTM -게이트(Gate)
- 장기 기억 통로일 뿐 인 “셀상태(Cell State)”를 실질적으로 컨트롤하는 중요한 역할
- 어떤정보를 기억/망각/사용할지를 ‘스스로학습’하여 결정
- 게이트의 작동 원리
- 모든 게이트는 시그모이드함수(Sigmoid Function)를 활용(0 ~ 1 사이의값으로 바꾸기)
- 0:게이트를 완전히 닫아 정보를 차단
- 1: 게이트를 완전히 열어 모든 정보를 통과
- 0.5: 게이트를 절반만 열어 정보의 50%만 통과(0.5 외의0 ~ 1 사이의 모든 값 가능)
- 시그모이드를 활용하는 이유: “흐름을 조절하기에 적합한 범위를 출력”
- (이전 시점의 단기 기억(h t−1) + 현재 시점의 새로운 정보(x t)) * 각 게이트의 가중치의 값으로 게이트를 얼마나 열지를 결정

# LSTM -게이트(Gate) –망각게이트(Forget Gate)
- “과거의 기억(장기기억)에서 어떤걸 지울까?” 를 결정 하는 게이트
- 불필요한 장기기억을 제거하는 역할
- 작동 방식
    1. 이전 시점의 은닉상태(h <sub>t-1</sub>)과 현재 시점의 입력(x<sub>t</sub>)를 하나의 벡터로 합침
    2. 망각게이트의 가중치(Wf)와 생성한 벡터를 곱함
        - "현재 상황을 고려 할 때, 장기 기억이 얼마나 중요한가?"를 평가하는 과정
    3. 시그모이드함수를 통과 시키면서 “망각벡터”가 생성
    4. 그리고 이 값을 셀 상태(장기기억)*망각벡터을 통해 셀상태(장기기억)을 선택적으로 지움

# LSTM -게이트(Gate) –입력게이트(Input Gate)
- “새로운 정보 중 어떤것을 셀 상태(장기기억)에 저장 할까?” 를 결정하는 게이트
- 중요하다고 판단되는 정보만 선별하여 기억을 업데이트하는 역할
- 작동 방식
    1. (동시진행)업데이트 벡터 생성(새로운정보를 반영할 비율을 결정)
        1. 이전 시점의 은닉상태(h<sub>t-1</sub>)과 현재 시점의 입력(x<sub>t</sub>)를 하나의 벡터로 합침
        2. 입력 게이트의 가중치(Wi)와 생성한 벡터를 곱함
        3. 시그모이드함수를 통과 시키면서 “업데이트벡터”가 생성
    1. (동시진행) 추가할 정보 생성
        1. 이전 시점의 은닉상태(h<sub>t-1</sub>)과 현재 시점의 입력(x<sub>t</sub>)를 하나의 벡터로 합침
        2. 셀의 가중치(W<sub>c</sub>)와 생성한 벡터를 곱함
        3. tanh를 통과 시키면서 “추가할정보”(tanh: 내용과 방향을 표현하는데 적합, -1 ~ 1)
    2. 그리고 이 값을 “추가할정보” * “업데이트벡터”을 통해 셀 상태(장기기억)에 업데이트


# LSTM -게이트(Gate) –출력게이트(Output Gate)
- 셀 상태(장기기억)의 모든 정보 중에서 예측헤 필요한 정보만 선별하여 외부로 내보내는 역할
- 셀의 최종결과를 결정하는 역할
- 작동 방식
    1. 출력벡터(마스크)생성( 정보선별마스크역할)
        1. 이전 시점의 은닉상태(h<sub>t-1</sub>)과 현재시점의 입력(x<sub>t</sub>)를 하나의 벡터로 합침
        2. 출력 게이트의 가중치(Wo)와 생성한 벡터를 곱함
        3. 시그모이드함수를 통과 시키면서 “출력벡터(마스크)”가 생성
    2. “셀상태” * “출력벡터(마스크)”을 통해 중요한 데이터만 출력
- 출력 게이트는 “셀상태”를 변경 하지 않음

# LSTM –학습방식
- LSTM학습 과정(오차를최소로하는가중치(W<sub>f</sub>, W<sub>i</sub>, W<sub>o</sub>, W<sub>c</sub>)를찾아가는과정)
1. 순전파-> 오차 계산(RNN과동일하게진행)
2. 역전파(Backpropagation Through Time, BPTT)
    - 은닉상태(단기기억)와 셀 상태(장기기억) 두 경로에 오차정보가 전달 됨
    - 은닉상태에 영향을 준건 “출력게이트(W<sub>o</sub>)”이므로, 출력게이트의 책임량을 구하면서 기존 RNN처럼 역전파를 진행(기울기 소실 여전)
    - 셀상태(장기기억)에 영향을 준건 “입력게이트(W<sub>i</sub>)” 와 “망각게이트(W<sub>f</sub>)” 이므로, 거슬러 올라가면서 “입력게이트”의 책임량, “망각게이트”의 책임량을 구하면서 역전파 진행
    - 중요)셀 상태는 복잡한 활성화함수를 거치지 않았고, 단순덧셈과 곱셈의 연산만 진행 했기 때문에, 미분을 해도 값이 “1”이 되면서, 오차값이 손실없이 계속 전달 됨
    - 이로 인해 기울기 소실 문제가 해결

# LSTM의 한계
- 고정된입출력문제
    - LSTM은 하나의 cell을 재사용하고 있고, 그렇기 때문에 입력 길이는 얼마든지 달라도 상관 없음
    - 대신, Cell이 1개 이기 때문에, 입력값에 대응하는 출력값도 1개가 나오게 됨
    - 고로 입력값을 3개를 넣으면, 3개의 값만 출력 할 수 있음
    - “나는학생이다”를 번역하게 된다면, “나는” => I, “학생이다” => am까지만 출력하게 되고 끝이 나게 됨
- 이 문제를 해결한 방식이 바로 **Seq2Seq**
    - (인코더) 입력 받는 LSTM
    - (디코더) 출력 하는 LSTM
    - “입력받는역할(인코더)”과 “출력하는역할(디코더)”를 나눠서 유연하게 대처 할 수 있도록 함