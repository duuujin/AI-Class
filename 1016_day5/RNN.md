# RNN(Recurrent Neural Network)
- 순환 신경망
- 순서가 있는 데이터(Sequential Data)를 처리하기 위해 고안된 인공신경망(Neural Network)
- 과거의 정보를 기억하고, 기억한 정보를 활용해 현재의 데이터를 해석
- “Cell” 이 입력데이터와 과거의데이터(은닉상태)를 섞어서 “새로운데이터”를 만들어냄

# RNN–Cell (1/4)
- 과거의 정보와 현재의 입력데이터를 섞어서 새로운 데이터를 만들어 내는 곳(like 퍼셉트론)
- 과거의 정보를 담아두는곳이 바로 “은닉상태(Hidden State)”
- 동작 방식
    1. 셀은 두 가지 데이터를 입력 받음
        - 현재 데이터(x<sub>t</sub>)
        - 이전셀의 은닉 상태(h<sub>t-1</sub>)
    2. 셀 내부에서 두 가지 데이터를 조합하여 새로운 은닉 상태를 만듦(h<sub>t</sub>)
    3. 새로운 은닉상태는 두 곳으로 전달 됨
        - 출력값(y<sub>1</sub>): 현재 시험의 예측결과(필요한경우에사용)
        - 다음셀: 다음 순서의 데이터를 처리 할 때 참고할 ‘기억’으로 전달

# RNN–Cell (2/4)
- 입력 데이터와 은닉 상태를 조합하는 방법에 대해서 살펴 보자.
- 각 입력 데이터와 은닉상태의 영향력을 나타내는 가중치가 존재
    - 입력 가중치(Wxh) : 입력 데이터의 영향력
    - 순환 가중치(Whh) : 이전 기억의 영향력
    - 초기에 해당 값은 랜덤으로 설정 됨
- 이후에 다시 언급 하겠지만
    - 모든 입력 가중치는 같은값
    - 모든 순환 가중치는 같은값
    - 입력 가중치와 순환 가중치가 같다는 말은 아님!!

# RNN–Cell (3/4)
- 입력 데이터와 은닉 상태 조합의 동작 방식
    1. 셀은 두가지 데이터를 그대로 받지 않고, 각자의 가중치를 곱해 중요도를 반영
        - 가중치가 반영된 현재 정보= 현재 데이터(x<sub>t</sub>) * 입력 가중치(W<sub>xh</sub>)
        - 가중치가 반영된 과거 기억= 이전 기억(h<sub>t-1</sub>) * 순환 가중치(W<sub>hh</sub>)
    2. 두 정보를 합침
        - 조합된 정보= 현재정보 + 과거기억
        - 실제로는 편향(default 값)을 추가로 더하지만, 단순화하기위해 생략
        - 각 데이터는 벡터(특징)로 표현 되며, 더하는것만으로도 새로운 특징을 만들어냄
    3. 합쳐진 정보를 활성화함수(tanh)에 넣어 압축
    4. 새로운 은닉상태(h<sub>t</sub>)를 다음 셀로 전달!

# [참고] 활성화함수-tanh
- 입력 받은 값을 -1과1 사이의 값으로 압축 시켜주는 역할
- 장점
    1. 출력값을 1 과-1 사이로 제한하여, 기울기폭발 방지
        - 이전 단계의 결과에 계속해서 같은 가중치를 곱하는구조이기 때문에 기울기 폭발 가능성이 있음
    2. 0을 중심으로 대칭이여서, 더 빠르고 안정적인 학습이 가능
        - 학습과정에서 가중치를 업데이트할 때, 모든 가중치가 같은 방향(모두증가/모두감소)으로 움직이는 편향이 줄어듬
- 여전히 기울기 소실이라는 한계가 있음
- RNN에서 다른 활성화함수를 쓰지 않는 이유
    - ReLU: 양수를 그대로 출력하기 때문에 기울기 폭발이 발생
    - 시그모이드(Sigmoid): 0과1 사이로 바꿔주면서, 0을 중심으로 하지않아서 학습이 느리고 장기기억을 못 함

# RNN–Cell (4/4)
- 사실 “셀(cell)은 하나다!”
- 이전에 말한 각각의 입력 데이터, 은닉 상태 가중치가 같은 이유
- 하나의 셀을 재사용 하는 이유(가중치공유)
    - 압도적인 효율성
        - 입력 데이터가 아무리 길어도, 학습 시킬 가중치세트가 하나임
        - 모델이 매우 가볍고 효율적
    - 좋은 일반화 성능
        - 각 셀이 정해진 순서에 따른 데이터를 학습 하는 것이 아님!! (ex: 첫번째 단어를 처리하는 방법, 세번재단어를 처리하는 방법, … )
        - 어떤 위치의 단어든 이전 기억과 조합하는 일반적인 방법을 학습
        - 훈련 때 학습 하지않은 길이의 문장에도 유연하게 대처 할 수 있음

# RNN의학습방식(1/2)
- RNN이 학습하는 과정(오차를 최소로 하는 가중치를 찾아가는 과정)
    1. 순전파(Forward Propagation)
        - 우리가 배웠던 과정을 그대로 진행
        - 랜덤하게 초기화된 가중치(Wxh, Whh)를 사용
        - 단어를 순서대로 셀을 통과시키면서 은닉상태를 업데이트
        - 각 시점에서, 혹은 마지막 시점에서 최종 예측값을 출력
    2. 오차 계산
        - 각 문제에 맞는 채점기준에 맞춰 오차를 계산
        - 예시) 품사분류기( 지도학습)
        - softmax: 클래스 총합이 1이 되도록, 각 확률을 구함
        - 학습과정에서는 softmax를 통과 후 나온 결과중 가장 높은 값과 정답을 비교해서 오차를 구함

# RNN의학습방식(2/2)
- RNN이 학습하는 과정(오차를 최소로 하는 가중치를 찾아가는 과정)
    1. 역전파(Backpropagation)
        - 정확히는 BPTT(Backpropagation Through Time)
        1. 최종 오차의 영향력을 역순으로 추적, 가중치가 오차에 미친 영향력을 구함
        2. 셀에는 은닉상태와 입력데이터의 가중치가 존재, 해당 시점의 “입력데이터의 가중치”가 오차에 미친 영향력을 구함
            - 우리가 원하는건 가중치를 업데이트 하는 것이다.
            - 기존 역전파와 마찬가지로 “입력데이터”에 따라 영향력(기울기)를 구함
        3. 해당 시점에서 최종오차가 “은닉상태가중치”와 “입력데이터가중치”로 적절하게 나뉘면, “은닉상태가중치의오차”를 이전 셀로 넘긴다.
        4. 2번~3번 과정을 첫번째 시점에 도달 할 때까지 반복 한다.
        5. RNN은 모든 시점에서 동일한 가중치를 사용하기 때문에 모든 시점의 가중치들을 합산하고,이를 학습률과 결합해 오차가 줄어드는 방향으로 가중치를 업데이트


# RNN의한계
- 기울기 소실 문제(Vanishing Gradient Problem)
    - BPTT(역전파) 과정에서 “이전정보의 책임” + “현재시점의 책임”으로 오차가 나눠지면서, 뒤로 갈수록 오차정보가 거의 전달되지 않음
    - 결국 오래된정보에 대해서 학습이 제대로 이루어지지 않음
    ➢ 장기 의존성 문제(Logn-Term Dependency Problem)
- 그리고 이와 같은 장기 의존성문제를 해결 하기 위해서 등장한것이 “LSTM(Logn Short-Term Memory)”