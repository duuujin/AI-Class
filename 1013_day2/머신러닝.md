# AI
- Arificial Intelligence
- 인간의 학습, 추론, 지각 능력 등을 컴퓨터를 통해 구현하는 가장 포괄적인 분야
- AI의 최종 목표는 기계가 사람처럼 보고, 듣고, 말하고, 생각하게 만드는 것
- AI를 구현하는 두 가지 접근법
    - 규칙 기반
        - 전문가의 지식과 규칙을 사람이 직접 컴퓨터에 코드로 입력하는 방식
        - 한계 : 세상의 모든 규칙을 만들 수 없고, 예외 상황에 대처하기 어려움
    - 학습 기반(머신러닝)
        - 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
        - 현대 AI의 핵심 동력이며, 우리가 배울 분야

# 머신 러닝
- 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
- 학습 종류
    - 지도 학습(Supervised Learning)
        - 정답지가 있는 데이터를 학습시키는 방법
        - 의료 진단, 스팸 필터링, 주가/부동산 가격 예측 등에 활용
    - 비지도 학습(Unsupervised Learning)
        - 정답이 없는 데이터의 숨겨진 구조나 규칙을 찾는 방법
        - 추천 시스템, 이상 거래 탐지, 고객 세분화 등에 활용
    - 강화 학습(reinforcement Learning)
        - '상'과 '벌'을 통해서 최적의 행동 방식을 학습하는 방법(알파고)
        - 로봇제어, 자율주행, 게임 AI 등에 활용
- 일반적으로 비지도 -> 지도 -> 강화 학습으로 복합적인 학습으로 결과물 생성

# 지도 학습(Supervised Learning)
- 정답(레이블,Label)이 있는 데이터(피쳐, Feature)를 학습시키는 방법
- 우리가 맞춰야 할 '정답'에 따라 두 가지의 큰 유형으로 나뉨
- 지도 학습 유형
    - 회귀(Regression) -> 회귀 문제를 푸는 가장 기본적이면서 강력한 도구가 '선형 회귀'
        - 연속적인 숫자 값을 예측하는 문제
        - Feature 와 Label 을 기반으로 학습한 모델이 새로운 Feature의 Label을 예측하는 것
        - ex) 공부한 시간에 따른 시험 점수 예측하기
    - 분류(Classification)
        - 주어진 Feature가 어떤 그룹(카테고리)에 속하는지 예측하는 문제
        - ex) 메일의 내용을 보고 '스팸'인지, '정상'인지 판단하기


# 선형 회귀(Iinear regression)
- 에측을 '하나의 직선'으로 하는 가장 간단하고 직관적인 방법(다차원일 경우에는 평면/초평면)
- (단순 선형 회귀) 하나의 특성(Feature)만 있을 때
    - 공부한 시간(x)에 따른 시험 점수(y) 예측
    - y = wx + b
    - w(가중치, Weight): 직선의 기울기. 공부 시간이 1시간 늘 때 점수가 몇 점오르는지를 나타냄
    - b(편향, Bias): y절편, 공부를 하나도 안 했을 때의 기본 점수를 의미
- (다중 선형 회귀) 여러 특성이 있을 때
    - 케럿(x1), 투명도(x2), 깊이(x3)를 모두 고려한 다이아몬드 가격(y) 예측
    - y =w1x1 + w2y2 + w3y3 + ... + b
    - 각각의 특성에 대한 최적의 가중치와 편향을 찾는 것이 목표

# 가설(Hypothesis)
- 우리가 예측에 사용할 “직선의방정식(모델)”을 지칭 하는 용어
- 그러면 어떤 가설(모델)이 좋은 가설일까?
    - 실제 데이터들과의 거리(오차)가 가장 작은 직선
    - 이 값은 어떻게 구할까?

# 비용함수(Cost Function)
- 모델(가설)이 얼마나 좋은 모델인지 나타내는점수(like 시간복잡도)
- 엄연히 다르긴 하지만 손실함수(loss function)라고도함
- 비용함수 종류
    - (회귀) 평균제곱오차(MSE, Mean Squared Error)
    - (회귀) 평균절대오차(MAE, Mean Absolute Error)
    - (분류) 교차엔트로피

# 평균제곱오차(MSE, Mean Squared Error) (1/2)
- 모델의 예측값이 실제 정답과 얼마나 차이나는지 측정하는 방법
- 점수가 낮을수록 모델의 성능이 좋다는 의미
- 계산방법
    1. 각 데이터들의 실제값과 예측값의 차이(오차)를 구한다.
    2. 각 데이터들의 오차를 모두 제곱하여 양수로 만들고 더하기
        ( 제곱하는이유: 양수오차와 음수오차의 상쇄 막기+ 큰 오차에 페널티 부여)
    3. 제곱총합을 데이터개수로 나누어 평균을 구함

# 평균제곱오차(MSE, Mean Squared Error) (2/2)
- 예시) 시험 성적 예측 모델
    - 학생A: 1시간 공부해서60점, 학생B: 2시간 공부해서70점, 학생C: 3시간 공부해서90점
    - 우리의 예측모델: 예측점수: 15* (공부시간) + 45( 15(가중치, w)와45(편향, b)는임의의초기값)
    - 학생A의 예측: 15 * 1 + 45 = 60 , 오차: 실제(60) –예측(60) = 0
    - 학생B의 예측: 15 * 2 + 45 = 75 , 오차: 실제(70) –예측(75) = (-5)
    - 학생C의 예측: 15 * 3 + 45 = 90 , 오차: 실제(90) –예측(90) = 0
    - 각 오차를 제곱하여 더한뒤, 학생수로나누기: ( 02+ (-5)2+ 02 ) / 학생수(3) = 25 / 3 = 8.33…
- 결국 우리의 목표는 MSE점수(비용)를 0에 가장 가깝게 만드는 최적의 모델 (w와 b)을 찾는 것!

# 최소제곱법(MLS, Method of Least Squares)
- 예측 모델(𝑦 = 𝑤𝑥 +𝑏)에서 비용을 최소화하는 w(가중치)와b(편향)을 찾는 원리
- 그러면 비용을 어떻게 최소화해서 최적의 모델을 찾을까?
- 최적의w 와b 를찾는방법
    1. 정규방정식(Normal Equation)
        - 복잡한 미분방정식을 풀어서 한번에 정답을 계산해내는 방법
        - 만들어진 수학공식을 이용해, 최적의 파라미터를 한번에 계산
    2. 경사하강법(Gradient Descent)
        - 최적의 파라미터를 점진적으로 찾아나가는 방법
        - 대부분의 모델이 이 방식을 채택