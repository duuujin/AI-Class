# AI
- Arificial Intelligence
- 인간의 학습, 추론, 지각 능력 등을 컴퓨터를 통해 구현하는 가장 포괄적인 분야
- AI의 최종 목표는 기계가 사람처럼 보고, 듣고, 말하고, 생각하게 만드는 것
- AI를 구현하는 두 가지 접근법
    - 규칙 기반
        - 전문가의 지식과 규칙을 사람이 직접 컴퓨터에 코드로 입력하는 방식
        - 한계 : 세상의 모든 규칙을 만들 수 없고, 예외 상황에 대처하기 어려움
    - 학습 기반(머신러닝)
        - 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
        - 현대 AI의 핵심 동력이며, 우리가 배울 분야

# 머신 러닝
- 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
- 학습 종류
    - 지도 학습(Supervised Learning)
        - 정답지가 있는 데이터를 학습시키는 방법
        - 의료 진단, 스팸 필터링, 주가/부동산 가격 예측 등에 활용
    - 비지도 학습(Unsupervised Learning)
        - 정답이 없는 데이터의 숨겨진 구조나 규칙을 찾는 방법
        - 추천 시스템, 이상 거래 탐지, 고객 세분화 등에 활용
    - 강화 학습(reinforcement Learning)
        - '상'과 '벌'을 통해서 최적의 행동 방식을 학습하는 방법(알파고)
        - 로봇제어, 자율주행, 게임 AI 등에 활용
- 일반적으로 비지도 -> 지도 -> 강화 학습으로 복합적인 학습으로 결과물 생성

# 지도 학습(Supervised Learning)
- 정답(레이블,Label)이 있는 데이터(피쳐, Feature)를 학습시키는 방법
- 우리가 맞춰야 할 '정답'에 따라 두 가지의 큰 유형으로 나뉨
- 지도 학습 유형
    - 회귀(Regression) -> 회귀 문제를 푸는 가장 기본적이면서 강력한 도구가 '선형 회귀'
        - 연속적인 숫자 값을 예측하는 문제
        - Feature 와 Label 을 기반으로 학습한 모델이 새로운 Feature의 Label을 예측하는 것
        - ex) 공부한 시간에 따른 시험 점수 예측하기
    - 분류(Classification)
        - 주어진 Feature가 어떤 그룹(카테고리)에 속하는지 예측하는 문제
        - ex) 메일의 내용을 보고 '스팸'인지, '정상'인지 판단하기


# 선형 회귀(Iinear regression)
- 에측을 '하나의 직선'으로 하는 가장 간단하고 직관적인 방법(다차원일 경우에는 평면/초평면)
- (단순 선형 회귀) 하나의 특성(Feature)만 있을 때
    - 공부한 시간(x)에 따른 시험 점수(y) 예측
    - y = wx + b
    - w(가중치, Weight): 직선의 기울기. 공부 시간이 1시간 늘 때 점수가 몇 점오르는지를 나타냄
    - b(편향, Bias): y절편, 공부를 하나도 안 했을 때의 기본 점수를 의미
- (다중 선형 회귀) 여러 특성이 있을 때
    - 케럿(x1), 투명도(x2), 깊이(x3)를 모두 고려한 다이아몬드 가격(y) 예측
    - y =w1x1 + w2y2 + w3y3 + ... + b
    - 각각의 특성에 대한 최적의 가중치와 편향을 찾는 것이 목표

# 가설(Hypothesis)
- 우리가 예측에 사용할 “직선의방정식(모델)”을 지칭 하는 용어
- 그러면 어떤 가설(모델)이 좋은 가설일까?
    - 실제 데이터들과의 거리(오차)가 가장 작은 직선
    - 이 값은 어떻게 구할까?

# 비용함수(Cost Function)
- 모델(가설)이 얼마나 좋은 모델인지 나타내는점수(like 시간복잡도)
- 엄연히 다르긴 하지만 손실함수(loss function)라고도함
- 비용함수 종류
    - (회귀) 평균제곱오차(MSE, Mean Squared Error)
    - (회귀) 평균절대오차(MAE, Mean Absolute Error)
    - (분류) 교차엔트로피

# 평균제곱오차(MSE, Mean Squared Error) (1/2)
- 모델의 예측값이 실제 정답과 얼마나 차이나는지 측정하는 방법
- 점수가 낮을수록 모델의 성능이 좋다는 의미
- 계산방법
    1. 각 데이터들의 실제값과 예측값의 차이(오차)를 구한다.
    2. 각 데이터들의 오차를 모두 제곱하여 양수로 만들고 더하기
        ( 제곱하는이유: 양수오차와 음수오차의 상쇄 막기+ 큰 오차에 페널티 부여)
    3. 제곱총합을 데이터개수로 나누어 평균을 구함

# 평균제곱오차(MSE, Mean Squared Error) (2/2)
- 예시) 시험 성적 예측 모델
    - 학생A: 1시간 공부해서60점, 학생B: 2시간 공부해서70점, 학생C: 3시간 공부해서90점
    - 우리의 예측모델: 예측점수: 15* (공부시간) + 45( 15(가중치, w)와45(편향, b)는임의의초기값)
    - 학생A의 예측: 15 * 1 + 45 = 60 , 오차: 실제(60) –예측(60) = 0
    - 학생B의 예측: 15 * 2 + 45 = 75 , 오차: 실제(70) –예측(75) = (-5)
    - 학생C의 예측: 15 * 3 + 45 = 90 , 오차: 실제(90) –예측(90) = 0
    - 각 오차를 제곱하여 더한뒤, 학생수로나누기: ( 02+ (-5)2+ 02 ) / 학생수(3) = 25 / 3 = 8.33…
- 결국 우리의 목표는 MSE점수(비용)를 0에 가장 가깝게 만드는 최적의 모델 (w와 b)을 찾는 것!

# 최소제곱법(MLS, Method of Least Squares)
- 예측 모델(𝑦 = 𝑤𝑥 +𝑏)에서 비용을 최소화하는 w(가중치)와b(편향)을 찾는 원리
- 그러면 비용을 어떻게 최소화해서 최적의 모델을 찾을까?
- 최적의w 와b 를찾는방법
    1. 정규방정식(Normal Equation)
        - 복잡한 미분방정식을 풀어서 한번에 정답을 계산해내는 방법
        - 만들어진 수학공식을 이용해, 최적의 파라미터를 한번에 계산
    2. 경사하강법(Gradient Descent)
        - 최적의 파라미터를 점진적으로 찾아나가는 방법
        - 대부분의 모델이 이 방식을 채택

# 정규방정식(1/2)
- 복잡한 미분 방정식을 풀어서 한번에 정답을 계산해내는 방법
- 만들어진 수학공식을 이용해, 최적의 파라미터를 한번에 계산
- θ (세타): 찾으려는 최종 목표, 모든 가중치(W)와 편향(b)을 담고 있는 결과값
- X: 입력데이터(feature) 행렬, y: 맞춰야할정답(레이블) 벡터
- 예시) 시험 성적 예측 모델
    - 학생A: 1시간 공부해서 60점, 학생B: 2시간 공부해서 70점, 학생C: 3시간 공부해서 90점
    - 예측점수 = 15 * (공부시간) + 43.33

# 정규방정식(2/2)
- 장점
    - 데이터의 특성(feature)가 적을때, 빠르고 정확하게 해를 찾음
    - 추가 설정이 필요 없음
- 단점
    - 데이터의 특성(feature)가 많으면 속도가 매우 느려짐
    - 역행렬이 존재하지 않으면 계산 할수 없음
    - 역행렬계산시 미세한 오차로 인해 계산결과가 완전히 틀어질 수 있음
- 이런 역행렬이 존재하지 않으면 계산 할 수 없는 문제는 “특이값분해(SVD)” 로 해결
- 특이값 분해(SVD)
    - 역행렬이없는 행렬을 역행렬이있는 가장 유사한 행렬을 만들어 내는것
    - 깊은 내용이니 진행X 

# 경사하강법(Gradient Descent)(1/2)
- 최적의 파라미터를 점진적으로 찾아나가는 방법
- 비용함수의 기울기가0 이되는 지점이 가장 비용이 낮은 지점
- 동작과정
    1. 랜덤한 지점에서 시작해서 현재지점의 “기울기”를 구한다.
    2. 해당 기울기만큼 움직인다.
        (움직이는보폭을“학습률(Learning Rate)” 라고함)
    3. 이동한 위치에서 다시 경사를 확인하고, 학습률만큼 이동한다.
    4. 위 과정을 계속 반복하면서, 가장 낮은 지점(기울기가0)에 도달

# 경사하강법(Gradient Descent)(2/2)
- 기울기만큼 이동(학습률)할때, 반대 방향으로 이동함(음수기울기면, 양수방향으로 이동해야 함)
- 학습률 설정
    - 학습률이 너무 크면, 보폭이 너무 커서 반대편으로 올라 갈 수 있음(발산, overshooting)
    - 학습률이 너무 작으면, 너무 오래 걸림
- 지역극솟값(local minima) 문제: 시작위치가 랜덤이기 때문에Global minimu이아닌local minimum에 빠질수있음

# 경사하강법(Gradient Descent)에서의용어/파라미터(1/2)
- 기울기(gradient)
    - 전체 데이터가 만들어내는 “평균적인오차”의 기울기
- 이터레이션(iterations)
    - 데이터의 일부 혹은 전체를 보고, 현재 위치의 경사를 계산하고, 한 걸음 이동하는 과정
- 에포크(Epoch)
    - 전체 데이터를 모두 한번 훑어보면, 1 에포크
- 학습률(Learning Rate, alpha)
    - 한 번에 얼마나 크게 움직일지를 결정 하는 보폭

# 경사하강법(Gradient Descent)에서의용어/파라미터(2/2)
- 허용오차(Tolerance, tol)
    - 학습 조기종료 조건
    - 모델의 오차 감소량이 매우 작아지면, 학습해도 성능향상이 없겠다고 판단하여 학습을 멈춤
- 배치크기(batch)
    - 기울기를 구할때, 전체 데이터오차의 기울기를 한 번에 구하지 않음 => 너무느림
    - 작은묶음(배치)로 나눠서 처리
    - 전체 데이터오차의 기울기를 한 번에 구하는 경우에는“1 이터레이션” = “1 에포크”
- 기울기 누적 스텝(Gradient Accumulation Steps)
    - 실질적인 배치 크기를 늘리는 고급 기법
    - 매 배치마다 업데이트 하는 것이 아닌, n 번의 배치간격으로 누적한 기울기를 기반으로 이동

# 경사하강법(Gradient Descent)의종류
1. 배치 경사 하강법(Batch Gradient Descent)
    - 전체 데이터를 훑어 보고, 기울기를 계산
    - 1 epoch = 1 iteration
    - 정확 하지만, 데이터가 크면 매우 느림
2. 확률적 경사 하강법(SGD , Stochastic Gradient Descent)
    - 하나의 데이터만보고 바로 기울기를 계산
    - 매우 빠르지만, 불안
3. 미니 배치 경사 하강법(Mini-batch Gradient Descent)
    - 적당한수의 배치(batch)만보고, 기울기를 계산
    - 속도와 안정성을 모두 잡은 방법
    - 배치수는 정해진 답이 없으며, 실험적으로 구해야함