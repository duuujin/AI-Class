# AI
- Arificial Intelligence
- 인간의 학습, 추론, 지각 능력 등을 컴퓨터를 통해 구현하는 가장 포괄적인 분야
- AI의 최종 목표는 기계가 사람처럼 보고, 듣고, 말하고, 생각하게 만드는 것
- AI를 구현하는 두 가지 접근법
    - 규칙 기반
        - 전문가의 지식과 규칙을 사람이 직접 컴퓨터에 코드로 입력하는 방식
        - 한계 : 세상의 모든 규칙을 만들 수 없고, 예외 상황에 대처하기 어려움
    - 학습 기반(머신러닝)
        - 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
        - 현대 AI의 핵심 동력이며, 우리가 배울 분야

# 머신 러닝
- 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 배우게 하는 방식
- 학습 종류
    - 지도 학습(Supervised Learning)
        - 정답지가 있는 데이터를 학습시키는 방법
        - 의료 진단, 스팸 필터링, 주가/부동산 가격 예측 등에 활용
    - 비지도 학습(Unsupervised Learning)
        - 정답이 없는 데이터의 숨겨진 구조나 규칙을 찾는 방법
        - 추천 시스템, 이상 거래 탐지, 고객 세분화 등에 활용
    - 강화 학습(reinforcement Learning)
        - '상'과 '벌'을 통해서 최적의 행동 방식을 학습하는 방법(알파고)
        - 로봇제어, 자율주행, 게임 AI 등에 활용
- 일반적으로 비지도 -> 지도 -> 강화 학습으로 복합적인 학습으로 결과물 생성

# 지도 학습(Supervised Learning)
- 정답(레이블,Label)이 있는 데이터(피쳐, Feature)를 학습시키는 방법
- 우리가 맞춰야 할 '정답'에 따라 두 가지의 큰 유형으로 나뉨
- 지도 학습 유형
    - 회귀(Regression) -> 회귀 문제를 푸는 가장 기본적이면서 강력한 도구가 '선형 회귀'
        - 연속적인 숫자 값을 예측하는 문제
        - Feature 와 Label 을 기반으로 학습한 모델이 새로운 Feature의 Label을 예측하는 것
        - ex) 공부한 시간에 따른 시험 점수 예측하기
    - 분류(Classification)
        - 주어진 Feature가 어떤 그룹(카테고리)에 속하는지 예측하는 문제
        - ex) 메일의 내용을 보고 '스팸'인지, '정상'인지 판단하기


# 선형 회귀(Iinear regression)
- 에측을 '하나의 직선'으로 하는 가장 간단하고 직관적인 방법(다차원일 경우에는 평면/초평면)
- (단순 선형 회귀) 하나의 특성(Feature)만 있을 때
    - 공부한 시간(x)에 따른 시험 점수(y) 예측
    - y = wx + b
    - w(가중치, Weight): 직선의 기울기. 공부 시간이 1시간 늘 때 점수가 몇 점오르는지를 나타냄
    - b(편향, Bias): y절편, 공부를 하나도 안 했을 때의 기본 점수를 의미
- (다중 선형 회귀) 여러 특성이 있을 때
    - 케럿(x1), 투명도(x2), 깊이(x3)를 모두 고려한 다이아몬드 가격(y) 예측
    - y =w1x1 + w2y2 + w3y3 + ... + b
    - 각각의 특성에 대한 최적의 가중치와 편향을 찾는 것이 목표

# 가설(Hypothesis)
- 우리가예측에사용할“직선의방정식(모델)”을지칭하는용어
- 그러면어떤가설(모델)이좋은가설일까?
    - 실제데이터들과의거리(오차)가가장작은직선
    - 이값은어떻게구할까?

# 비용함수(Cost Function)
- 모델(가설)이얼마나좋은모델인지나타내는점수(like 시간복잡도)
- 엄연히다르긴하지만손실함수(loss function)라고도함
- 비용함수종류
    - (회귀) 평균제곱오차(MSE, Mean Squared Error)
    - (회귀) 평균절대오차(MAE, Mean Absolute Error)
    - (분류) 교차엔트로피

# 평균제곱오차(MSE, Mean Squared Error) (1/2)
- 모델의예측값이실제정답과얼마나차이나는지측정하는방법
- 점수가낮을수록모델의성능이좋다는의미
- 계산방법
    1. 각데이터들의실제값과예측값의차이(오차)를구한다.
    2. 각데이터들의오차를모두제곱하여양수로만들고더하기
        ( 제곱하는이유: 양수오차와음수오차의상쇄막기+ 큰오차에페널티부여)
    3. 제곱총합을데이터개수로나누어평균을구함

# 평균제곱오차(MSE, Mean Squared Error) (2/2)
- 예시) 시험성적예측모델
    - 학생A: 1시간공부해서60점, 학생B: 2시간공부해서70점, 학생C: 3시간공부해서90점
    - 우리의예측모델: 예측점수: 15* (공부시간) + 45( 15(가중치, w)와45(편향, b)는임의의초기값)
    - 학생A의예측: 15 * 1 + 45 = 60 , 오차: 실제(60) –예측(60) = 0
    - 학생B의예측: 15 * 2 + 45 = 75 , 오차: 실제(70) –예측(75) = (-5)
    - 학생C의예측: 15 * 3 + 45 = 90 , 오차: 실제(90) –예측(90) = 0
    - 각오차를제곱하여더한뒤, 학생수로나누기: ( 02+ (-5)2+ 02 ) / 학생수(3) = 25 / 3 = 8.33…
- 결국 우리의 목표는 MSE점수(비용)를 0에 가장 가깝게 만드는 최적의 모델 (w와 b)을 찾는 것!

# 최소제곱법(MLS, Method of Least Squares)
- 예측모델(𝑦 = 𝑤𝑥 +𝑏)에서비용을최소화하는w(가중치)와b(편향)을찾는원리
- 그러면비용을어떻게최소화해서최적의모델을찾을까?
- 최적의w 와b 를찾는방법
    1. 정규방정식(Normal Equation)
        - 복잡한미분방정식을풀어서한번에정답을계산해내는방법
        - 만들어진수학공식을이용해, 최적의파라미터를한번에계산
    2. 경사하강법(Gradient Descent)
        - 최적의파라미터를점진적으로찾아나가는방법
        - 대부분의모델이이방식을채택