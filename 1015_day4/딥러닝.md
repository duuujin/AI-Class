# 머신 러닝
- 컴퓨터에게 데이터를 주고, 그 데이터 안에서 규칙(패턴)을 스스로 학습하게 하는 방식
- 학습 종류
    - 지도 학습(Supervised Learning)
    - 비지도 학습(Unsupervised Learning)
    - 강화 학습(reinforcement Learning)

- 고전적인 머신러닝의 한계
    - 모델이 잘 학습할 수 있도록 사람이 직접 데이터의 중요한 부분을 알려주고, 가공해서 먹여줘야 함
    - 데이터의 양이 일정 수준을 넘어서면, 복잡한 패턴ㅇ르 추가로 학습하지 못하고 성능이 정체됨
    - 기본적으로 행과 열이 정해진 정형 데이터 처리에 적합하여, 이미지나 텍스트, 음성과 같은 비정형 데이터 처리에 비효율적
- 위 한계를 극복하기 위해서 등장한 방법론이 **딥러닝**

# 딥러닝(Deep Learning) 아이디어
- 인간의 뇌에서 영감을 받은 인공신경망을 사용하여 복잡한 패턴을 학습하는 머신러닝 학습 방법론
- 핵심 아이디어
    - 뇌의 신경망 구조와 동작 방식을 컴퓨터로 모방
    - 뇌가 수십억 개의 신경세포(뉴런)들이 서로 신호를 주고 받으며 학습하고 판단하는 것처럼
    - 컴퓨터 안에 인공뉴런을 만들고, 이들을 연결하여 네트워크를 구축
- 인공 신경망(ANN, Artificial Neural Network)
    - 컴퓨터 안에 인공뉴런(퍼셉트론)을 만들고, 이들을 연결하여 네트워크를 구축한 것

# 퍼셉트론(Perceptron)(1/3)
- 여러 정보를 받아서 최종적으로 참(1) 또는 거짓(0)과 같은 결정을 내리는 간단한 결정 모델
- 인공 신경망(ANN)의 가장 기본적인 단위로, 사람의 뇌를 구성하는 뉴런의 작동 원리를 모방한 알고리즘
- 구성 요소
    - 입력(inputs, x)
    - 가중치(weights, w)
    - 가중합(Weighted Sum)
        - 모든 입력에 각각의 가중치를 곱한 뒤, 모두 더한 값 + 편향
        - (x1 * w1) + (x2 * w2) + ... + b 와 같이 **선형 회귀**와 유사
    - 활서오하 함수(Activation Funcion)
        - 입력된 신호를 받아, 그 신호를 처리하여 출력 신호를 결정하는 함수
        - 즉, **활성화**시켜 전달할지 말지를 결정하는 스위치

# 퍼셉트론(Perceptron)(2/3)
- 퍼셉트론은 결국 데이터 사이에 "직선"하나를 그어서 두 그룹을 나누는 것과 같음
    - 가중합(z) = (x1 * w1) + (x2 * w2) + ... + b
    - z가 0보다 크면, 1 0보다 작으면 0으로 분류
    - 즉 z = (x1 * w1) + (x2 * w2) + ... + b가 되는데, 수학적인 이유로 1차 함수를 의미하게 됨
- 그리고 퍼셉트론은 "선형 분리 가능한 문제"를 잘 해결했음
- 선형 분리 가능한 문제
    - 데이터들을 하나의 직선으로 완벽하게 나눌 수 있는 문제
    - 예) 논리 게이트 (AND, OR)
    - 예) 간단한 분류 문제(스팸 메일 분류, 암 진단)
- 기계가 데이터를 보고 스스로 경계선(규칙)을 찾아낼 수 있다는 가능성을 보여준 엄청난 알고리즘

# 퍼셉트론(Perceptron)(3/3)
- "XOR(베타적 논리합) 문제"를 해결할 수 없는 치명적인 문제를 가지고 있음
    - XOR : 두 입력이 서로 다를 때만 1(참), 같으면 0(거짓)
    - "가격은 싼데(T) 리뷰는 좋거나(T) , 가격은 비싼데(F) 리뷰는 안 좋은 곳(F)이 진짜 숨은 맛집(F)이다!
- XOR 문제 때문에 퍼셉트론은 근본적인 한계가 있다는 비판과 함께 AI의 겨울이 도래..
- 하지만, 퍼셉트론을 여러 층으로 쌓아 직선이 하나가 아닌, 여러 개의 직선을 조합하여 XOR 문제를 해결
    - 신경망(Neural Network)의 시작

# 얕은 신경망(Shallow Neural Network, SNN)
- 초기 인공 신경망의 형태
- 얕은 신경망(Shallow Neural Network)
    - 입력/은닉/출력의 3가지 계층으로 이루어짐
    - 은닉층(Hidden Layer)가 딱 하나만 있는 구조
    - **만능 근사 이론**에 의해서 이론적으로는 모든 문제를 해결할 수 있음
- 만긍 근사 이론
    - 이론적으로 은닉층에 뉴런이 충분히 많다면 단 하나의 은닉층으로만 세상의 거의 모든 연속적인 함수를 흉내 낼 수 있음
    - 하지만, 은닉층을 터무니 없이 넓게 만들어야했고(계산량 매우 큼), 단계별/조합적 학습이 불가능
- 그래서 뉴런을 넓게 만드는 것이 아니라, 깊게 쌓아올리는 아이디어가 "심층 신경망"

# 심층 신경망(Deep Neural Network, DNN) ~ 딥러닝
- 신경망을 옆으로가 아닌, 깊게(Deep) 쌓는 방식
- 여러 개의 은닉층을 가진 인공 신경망
- 얕은 신경망과 비교
    - 얕은 신경망: 뉴런 12개 -> 조합 12개(재사용 불가능)
    - 깊은 신경망: 뉴런 6개 * 6개 -> 조합 36개
- 더 복잡하고 고차원적인 특징을 학습 가능
- 사람이 특징을 알려주지 않아도, 복잡한 패턴을 스스로 발견
- 정리하자면, "심층 신경망"이라는 모델을 사용하여 딥러닝을 진행!
