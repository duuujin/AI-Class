# 다중 퍼셉트론(Multi-Layer Perceptron, MLP)
- 인공 신경망(Neural Network)의 한 종류로, 퍼셉트론을 여러 층으로 쌓은 모델
- 입력층과 출력층 사이에 1개 이상의 은닉충을 가짐
- MLP의 학습 과정
    1. 순전파(Forward Propagation)
    2. 손실함수계산(Loss Function)
    3. 역전파(Back Propagation) 및가중치업데이트(Optimization)
    4. 1~3번반복후종료(Training 완료)

# MLP 학습과정–순전파(Forward Propagation)
- 데이터가 입력층에서 시작해서 여러 은닉층을 거쳐 출력층까지, 앞에서 뒤로 흘러가면서 계산되는 과정
- 순전파과정
    1. 초기에 각 뉴런(퍼셉트론)들은 무작위 가중치를 갖음
    2. 데이터(특징벡터)가 연결된 입력층으로 들어감
    3. 각 뉴런은 입력값과 무작위로 초기화된 가중치를 이용해 계산하고,
        활성화함수(RELU)를 통과시켜 결과를 다음층으로 전달
    4. 마지막 출력층까지 반복되어 최종 예측 값을 출력
- 초기에는 가중치가 랜덤이기때문에, 매우 엉터리결과가 나옴

# MLP 학습과정–손실함수계산(Loss Function)
- 순전파 과정에서 나온 결과를 실제 정답과 비교해서 손실(Loss)/오차(Error)를 구함
- 구하는 방법
    - 회귀( 평균제곱오차, 각오차의제곱의합)
    - 분류(교차엔트로피, 모델이 강하게 확신하고 틀리면 강한 벌점)
    - 비지도(클러스터링 후 떨어진 거리의 총합을 줄이기, 심플버전)

# MLP 학습과정–역전파(Back Propagation)
- 손실값을 가지고, 출력층에서 거꾸로 계산하면서 가중치를 업데이트하는 과정
- 역전파 과정(쉬움버전)
    1. 발생한 오차를 가지고, 이전층으로 거꾸로 추적하면서
        각 가중치가 오차에 준 영향을 구함
    2. 계산된 영향력을 바탕으로 오차를 줄이는 방향으로
        각 가중치를 조금씩 업데이트
    3. 첫 은닉층까지 반복 후 다시 순전파 반복
- 이해가 어려우면 아래 링크를 참고 하는것도 추천
    - https://www.youtube.com/watch?v=tkH7KgLZc0E

# [심화] 역전파진행과정 (1/2)
1. 예측값과 정답 차이에서 하나의 "최종오차(L)"가 발생
2. "최종오차(L)"가 뒤로 전파 되면서, 각 뉴런은 최종오차(L)에 대한 각기 다른 '책임신호'를 전달 받는다.
    - 책임 신호는 뉴런의 출력값이 최종오차(L)에 얼마나 영향을 미쳤는지를 나타냄
    - 수학적으로는 “최종오차(L)”를 “각뉴런의출력”으로 편미분
3. 뉴런 내부의 각(가중치(w)×입력데이터(x))은 책임신호에 각기 다른 영향을 미쳤고, 이 영향력에 맞춰서 가중치를 수정 해야한다.
    - 영향력이 크면 가중치 수정도 더 많이 해야한다.

# [심화] 역전파진행과정(2/2)
3. 뉴런 내부의 각(가중치(w)×입력데이터(x))은 책임 신호에 각기 다른 영향을 미쳤고, 이 영향력에 맞춰서 가중치를 수정 해야한다.
    - 영향력이 크면 가중치수정도 더 많이 해야한다.
4. (가중치(w)*입력데이터(x))의 영향력에서 실질적으로 책임 신호에 영향을 주는부분은 "입력데이터(x)"이다.
    - 우리가 궁금한건 “가중치를 조절했을때의 결과가 얼마나 변할까? 이고, 이 가중치의 영향력을 결정하는것은 입력데이터의 몫
    - 예를들어) 방정식b = 7 * a 가있을때, a의영향력 보고 싶으면 결국 a를 고정하고 7을 쳐다봐야한다.
    - 그리고 이것이 미분(편미분)이다…!
5. 그렇기에 기존에 있던 가중치(w)는 신경 쓰지말고, 입력데이터(x)과 책임 신호를 곱해서 최종책임신호(가중치의기울기)를 계산 후 학습률과 곱해서 각 가중치값을 업데이트한다.
    - 입력데이터가↑ -> 영향력이↑ -> 가중치의 업데이트 변화량도 커야하고-> 학습률이랑 곱해질 최종 책임 신호값도 커야한다

# MLP 학습과정–Traning 완료
- 순전파-> 오차계산-> 역전파 과정을 언제까지 진행할까?
1. 정해둔 학습횟수(Epoch)에 도달
    - 언제 끝날지 예측이 가능하고, 구현이 간단
    - 너무 적게 반복하면 과소적합, 너무 많이 반복하면 과적합
2. 조기종료(Early Stopping) –가장 많이 쓰이는 방법
    1. 훈련/검증/테스트데이터로 나눈 후, 훈련데이터로 가중치를 업데이트
    2. 1 Epoch가 끝날때마다, 검증 데이터로 오차를 확인
    3. 훈련이 지속 될수록 오차가 줄어들다가, 어느순간 과적합으로 인해 오차가 떨어지기 시작
    4. 여기서 연속으로 N번이상 오차의 변화가 없거나 떨어지면 학습을 중단
    - 모델이 가장 성능이 좋았던 ‘최적의순간’에 학습을 멈추게 해줌