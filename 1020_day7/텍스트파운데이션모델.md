# 텍스트 파운데이션 모델
- GPT-1, BERT와 같은 언어 모델에도 3가지 구성 요소가 이미 포함되어 있음
- GPT-2: 언어 모델이 추가 학습 없이도 텍스트 지시를 통해 새로운 테스크를 어느 정도 수행할 수 있음을 확인
- 특이점
    - 규모의 법칙(Scaling Law): 더 많은 데이터, 큰 모델, 긴 학습 -> 더 좋은 성능
    - 창발성(Emergent Property): 특정 규모를 넘어서면 갑자기 모델에서 발현되는 성질
- 기존 대비, (1) 더 큰 모델(>7B)이 (2) 더 많은 데이터(>1T)에서 학습되어 창발성이 나타나기 시작한 언어 모델

# 거대 언어 모델 예시(폐쇄)
- 폐쇄형 (Closed) 거대 언어 모델
- ChatGPT(OpenAI) : 가장 많은 활성 유저 수, 전반적으로 뛰어난 성능
- Claude(Anthropic) : 안전 지향적 모델, 코딩 관련 작업에 특히 뛰어난 성능
- Gemini(Google) : 가장 긴 입력 및 출력(>1M)을 지원, 뛰어난 멀티 모달 기능
- 장점 : 일반적으로 더 우수한 성능 및 최신 기능을 갖고 있으며 사용하기 쉬움
- 단점 : 사용 시 마다 비용이 발생, 모델이나 출력에 대한 정보가 제한적으로 제공됨

# 거대 언어 모델 예시(개방)
- 개방형 (Open sourced) 거대 언어 모델
- LLaMA (Meta), Gemma (Google), Qwen (Alibaba)
- 장점 : 무료로 다운로드 및 사용 가능, 모든 정보(모델구조, 소스코드)가 공개되어 있음
- 단점 : 충분한 계산 자원(ex : GPU) 필요, 상대적으로 폐쇄형 모델에 비해 성능이 낮은 편

# 거대 언어 모델의 학습
- 정렬(Alignment) 학습 : 거대 언어 모델의 출력이 사용자의 의도와 가치를 반영하도록 하는 것
    - (1) 지시 학습 (instruction tuning) : 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
    - (2) 선호 학습 (preference learning) : 상대적으로 어떤 응답이 더 선호되어야 하는지

# 지시 학습 (Instruction tuning)
- 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
- 학습 방법 자체는 기존 언어 모델 (e.g.BERT)에서의 지도 추가 학습 (supervised fine-tuning, SFT)와 동일
- Remark. 기존 언어 모델은 각 테스크(e.g.감정 분석) 마다 별도의 추가 학습 및 모델을 저장
- Idea: 모든 자연어 테스크는 텍스트 기반 지시(instruction)와 응답로 표현할 수 있지 않을까?
- 학습 방법 : 주어진 입력을 받아서 이에 대한 응답을 따라 하도록 지도 추가 학습(Supervised Fine-Tuning, SFT)
- 학습 데이터의 다양성 증대를 위해, 각 테스크를 다양한 지시(템플릿)로 표현할 수 있음
- 기존 NLP 테스크 데이터를 지시 학습을 위한 데이터로 수정하여 학습 및 테스트에 활용
- 학습시에 보지 못한 지시에 대한 일반화 성능 평가를 위해, 관련 없는 테스크들을 테스트에 별도로 활용
- 실험 결과 : 예시 없어도 (0-shot) 새로운 지시에 대해 올바른 응답을 내놓는 성능이 크게 증가!
- 실험 결과 : 성능 향상을 위한 핵심 요소는 다음과 같음
    1. 학습 테스크의 개수: 다양한 종류의 지시를 학습할 수록 보지 못한 지시에 대한 일반화 성능이 좋아짐
    2. 추가 학습하는 모델의 크기 : 특정 규모 이하에서는 지시 학습의 효과성이 떨어짐 -> 지시를 이해하고 응답하는 것도 창발성의 하나
    3. 지시를 주는 방법 : 자연어 지시로 사람에게 대화하듯 지시하는 것이 가장 효과적

# 선호 학습 (Preference Learning)
- 다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가학습
- 다양한 응답은 모델이 생성, 응답간의 선호도는 사람이 제공
- ChatGPT를 만들기 위한 핵심 알고리즘
- InstructGPT의 핵심 아이디어 : 사람의 피드백을 통한 강화학습(Reinforcement Learing from Human Feedback, RLHF)
- InstructGPT의 학습 방법 : Step 1 , 지시 학습을 통한 텍스트 파운데이션모델(e,g,GPT-3)의 추가 학습
    - 실제 유저로부터 다양한 지시 입력을 수집하고, 해당 입력에 대해 훈련된 사람 주석자들이 정잡 데이터를 생성
- InstructGPT의 학습 방법 : Step 2 , 사람의 선호 데이터를 수집하여, 보상 모델(Reward model, RM)을 학습
    - 주어진 입력에 대한 선택지는 모델이 생성, 다양한 선택지에 대한 선호도는 사람이 생성
    - 사람과 일치한 선호도를 출력할 수 있도록 보상 모델을 지도 학습
- InstructGPT의 학습 방법 : Step 3 , 보상이 높은 응답을 생성하도록 강화 학습을 통해 추가 학습
    - 핵심: Step 1&2에서 보지 못한 질문에 대해 사람의 추가적인 개입 없이 학습된 모델들을 통해 추가 학습이 진행
    - 지시 학습된 모델을 보상 모델 기반 강화 학습을 통해 한번 더 추가 학습
- InstructGPT의 결과 : 유저의 지시를 얼마나 잘 수행하는 지를 사람이 직접 평가
    - 단순 프롬프팅이나 지시 학습에 비해 발전된 지시 수행능력을 보여줌
    - 기존 대비, InstructGPT는 해로운 응답(RalToxicity)과 거짓말(TruthfulQA, Hallucinations)을 덜 생성
- LLaMA2
    - InstructGPT와 비슷하게 RLHF와 대화 데이터를 활용한 LLaMA2 Chat 모델을 공개
    - 당시 대화형 타입 개방형 거대 언어 모델 중 가장 우수한 성능을 보여줌

