# 거대 언어 모델의 학습
- 정렬(Alignment) 학습 : 거대 언어 모델의 출력이 사용자의 의도와 가치를 반영하도록 하는 것
    - (1) 지시 학습 (instruction tuning) : 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
    - (2) 선호 학습 (preference learning) : 상대적으로 어떤 응답이 더 선호되어야 하는지

# 지시 학습 (Instruction tuning)
- 주어진 지시에 대해 어떤 응답이 생성되어야 하는지
- 학습 방법 자체는 기존 언어 모델 (e.g.BERT)에서의 지도 추가 학습 (supervised fine-tuning, SFT)와 동일
- Remark. 기존 언어 모델은 각 테스크(e.g.감정 분석) 마다 별도의 추가 학습 및 모델을 저장
- Idea: 모든 자연어 테스크는 텍스트 기반 지시(instruction)와 응답로 표현할 수 있지 않을까?
- 학습 방법 : 주어진 입력을 받아서 이에 대한 응답을 따라 하도록 지도 추가 학습(Supervised Fine-Tuning, SFT)
- 학습 데이터의 다양성 증대를 위해, 각 테스크를 다양한 지시(템플릿)로 표현할 수 있음
- 기존 NLP 테스크 데이터를 지시 학습을 위한 데이터로 수정하여 학습 및 테스트에 활용
- 학습시에 보지 못한 지시에 대한 일반화 성능 평가를 위해, 관련 없는 테스크들을 테스트에 별도로 활용
- 실험 결과 : 예시 없어도 (0-shot) 새로운 지시에 대해 올바른 응답을 내놓는 성능이 크게 증가!
- 실험 결과 : 성능 향상을 위한 핵심 요소는 다음과 같음
    1. 학습 테스크의 개수: 다양한 종류의 지시를 학습할 수록 보지 못한 지시에 대한 일반화 성능이 좋아짐
    2. 추가 학습하는 모델의 크기 : 특정 규모 이하에서는 지시 학습의 효과성이 떨어짐 -> 지시를 이해하고 응답하는 것도 창발성의 하나
    3. 지시를 주는 방법 : 자연어 지시로 사람에게 대화하듯 지시하는 것이 가장 효과적

# 선호 학습 (Preference Learning)
- 다양한 응답 중 사람이 더 선호하는 응답을 생성하도록 추가학습
- 다양한 응답은 모델이 생성, 응답간의 선호도는 사람이 제공
- ChatGPT를 만들기 위한 핵심 알고리즘
- InstructGPT의 핵심 아이디어 : 사람의 피드백을 통한 강화학습(Reinforcement Learing from Human Feedback, RLHF)
- InstructGPT의 학습 방법 : Step 1 , 지시 학습을 통한 텍스트 파운데이션모델(e,g,GPT-3)의 추가 학습
    - 실제 유저로부터 다양한 지시 입력을 수집하고, 해당 입력에 대해 훈련된 사람 주석자들이 정잡 데이터를 생성
- InstructGPT의 학습 방법 : Step 2 , 사람의 선호 데이터를 수집하여, 보상 모델(Reward model, RM)을 학습
    - 주어진 입력에 대한 선택지는 모델이 생성, 다양한 선택지에 대한 선호도는 사람이 생성
    - 사람과 일치한 선호도를 출력할 수 있도록 보상 모델을 지도 학습
- InstructGPT의 학습 방법 : Step 3 , 보상이 높은 응답을 생성하도록 강화 학습을 통해 추가 학습
    - 핵심: Step 1&2에서 보지 못한 질문에 대해 사람의 추가적인 개입 없이 학습된 모델들을 통해 추가 학습이 진행
    - 지시 학습된 모델을 보상 모델 기반 강화 학습을 통해 한번 더 추가 학습
- InstructGPT의 결과 : 유저의 지시를 얼마나 잘 수행하는 지를 사람이 직접 평가
    - 단순 프롬프팅이나 지시 학습에 비해 발전된 지시 수행능력을 보여줌
    - 기존 대비, InstructGPT는 해로운 응답(RalToxicity)과 거짓말(TruthfulQA, Hallucinations)을 덜 생성
- LLaMA2
    - InstructGPT와 비슷하게 RLHF와 대화 데이터를 활용한 LLaMA2 Chat 모델을 공개
    - 당시 대화형 타입 개방형 거대 언어 모델 중 가장 우수한 성능을 보여줌

