# CLIP(2021)
- Contrastive Language-Image Pre-training, by OpenAI
- AI가 언어와 시각을 통합해서 이해하는 방식을 보여준 패러다임 전환 제시
- 파운데이션 모델으로써의 특징
    - 입력 : 학습하지 않은 새로운 도메인의 입력 데이터에 대해서도 좋은 성능을 발휘, 제로샷 전이
    - 출력 : 자연어를 이용해 한 번도 본 적 없는 카테고리도 텍스트 설명만으로 출력 정의 가능, 언어 인터페이스
- 대조 학습 기반(Contrastive Pre-training)의 언어-이미지 사전 학습
    - 인터넷 데이터를 통한 지도 학습(supervised learning)을 통해 자연어 기반 시각 개념 학습
    - 다양한 이미지-자연어 쌍으로 학습
# CLIP 구조 - 텍스트 인코더(Transformer 기반 Text Encoder)
- Remind - Transformer
    - 트랜스포머 구조 = 인코더(Encoder) + 디코더(Decoder)
- CLIP에서는 Encoder only 구조 사용
- 토큰이라는 단위의 입력
- 입력된 토큰 간의 관계성을 집중하는 Attention 메커니즘으로 구성
- L 길이의 입력 토큰은 D-차원 특징벡터(임베딩)의 배열로 형태로 입력(L * D)
- 자연어 데이터: Sub-word 단위의 임베딩

# CLIP 구조 - 이미지 인코더(ViT : Vision Transformer, 2020)
- Remind - Vision Transformer
    - 입력구성
        - 텍스트 인코더 (자연어 데이터 입력) : Sub-word 단위의 임베딩
        - 이미지 인코더 (이미지 데이터 입력) : 패치 단위의 임베딩
    - ViT : 비전 분야에 트랜스포머를 (최소 수정으로) 적용한 모델
- 이미지를 작은 패치(16*16*3)로 나눔
- 각 패치를 1D로 Flatten
- Learnable position embedding 사용
    - 이미지 내에서 각 패치의 위치 민감 정보 추가
    - 모델 학습 과정에서 함께 학습됨
- Transformer encoder : 패치 처리
- MLP Head를 통해 분류 작업 수행

# CLIP 학습
- 대조 학습(Contrastive learning)
    - 학습기준
        - 목표 이미지(앵커)를 대응하는 텍스트(양성)와 가깝게
        - 일치하지 않는 여러 텍스트(음성)와는 멀게

# CLIP 간단 응용
- 제로샷 이미지 인식기
    - 텍스트로 원하는 물체 카테고리 리스트 준비
    - 텍스트 기반 카테고리 리스트를 텍스트 임베이딩으로 변환하여 Vector DB준비
    - 쿼리 이미지와 비교해서 가장 높은 점수의 카테고리 반환
    - 생각해보기
        - 검색 시스템과 유사성은 무엇일까?
        - 카테고리 이외에 어떤 것이 가능할까?
        - 카테고리가 정말 많은 경우에 어떻게 효율화 할까?

# 요약 및 정리
- 파운데이션 모델은 대규모 데이터로 사전학습된 범용 모델
- 파운데이션 모델의 방대한 사전 지식을 이용해 다양한 태스크에 빠르게 적용 가능
    - 장점 : 데이터/리소스 효율적, 범용성, 확장성, 높은 성능
- 파운데이션 모델은 대규모, 적응성, 범용성의 특징을 가짐
- 대표 이미지 파운데이션 모델
    - 이넡넷 상의 대규모 {텍스트,이미지} 페어 데이터를 활용한 이미지-언어 연관성을 학습한 CLIP 파운데이션 모델과 그 제로샷 이미지 인식기 응용